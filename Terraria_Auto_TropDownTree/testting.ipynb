{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision.transforms import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_video(video_path):\n",
    "    # Open the video file\n",
    "    video = cv2.VideoCapture(video_path)\n",
    "    \n",
    "    # Get the video properties\n",
    "    width = int(video.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(video.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    fps = video.get(cv2.CAP_PROP_FPS)\n",
    "    \n",
    "    # Check if the video properties meet the requirements\n",
    "    if width != 1280 or height != 720:\n",
    "        print(\"Video resolution is not 720p. Please provide a video with resolution 1280x720.\")\n",
    "        return None\n",
    "    \n",
    "    if fps != 29.97:\n",
    "        print(\"Video frame rate is not 29.97 fps. Please provide a video with frame rate 29.97.\")\n",
    "        return None\n",
    "    \n",
    "    # Preprocess the video frames\n",
    "    frames = []\n",
    "    while True:\n",
    "        ret, frame = video.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        \n",
    "        # Perform preprocessing on the frame (e.g., resize, normalize, etc.)\n",
    "        # ...\n",
    "        \n",
    "        frames.append(frame)\n",
    "    \n",
    "    # Release the video file\n",
    "    video.release()\n",
    "    \n",
    "    return frames\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class VoYAGER(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(VoYAGER, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.relu1 = nn.ReLU(inplace=True)\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1)\n",
    "        self.relu2 = nn.ReLU(inplace=True)\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        \n",
    "        self.conv3 = nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1)\n",
    "        self.relu3 = nn.ReLU(inplace=True)\n",
    "        self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        \n",
    "        self.fc1 = nn.Linear(256 * 4 * 4, 1024)\n",
    "        self.relu4 = nn.ReLU(inplace=True)\n",
    "        self.dropout = nn.Dropout(p=0.5)\n",
    "        \n",
    "        self.fc2 = nn.Linear(1024, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.pool1(x)\n",
    "        \n",
    "        x = self.conv2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.pool2(x)\n",
    "        \n",
    "        x = self.conv3(x)\n",
    "        x = self.relu3(x)\n",
    "        x = self.pool3(x)\n",
    "        \n",
    "        x = x.view(x.size(0), -1)\n",
    "        \n",
    "        x = self.fc1(x)\n",
    "        x = self.relu4(x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the dataset class\n",
    "class VideoDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, video_dir, transform=None):\n",
    "        self.video_dir = video_dir\n",
    "        self.video_files = os.listdir(video_dir)\n",
    "        self.transform = transform\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.video_files)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        video_path = os.path.join(self.video_dir, self.video_files[idx])\n",
    "        frames = preprocess_video(video_path)\n",
    "        \n",
    "        if self.transform:\n",
    "            frames = self.transform(frames)\n",
    "        \n",
    "        return frames\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Set the random seed for reproducibility\n",
    "    random.seed(42)\n",
    "    torch.manual_seed(42)\n",
    "    \n",
    "    # Define the data directory\n",
    "    data_dir = \"/path/to/data\"\n",
    "    \n",
    "    # Define the transformations for preprocessing\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        # Add more transformations as needed\n",
    "    ])\n",
    "    \n",
    "    # Create the dataset\n",
    "    dataset = VideoDataset(data_dir, transform=transform)\n",
    "    \n",
    "    # Split the dataset into train, validation, and test sets\n",
    "    train_size = int(0.7 * len(dataset))\n",
    "    val_size = int(0.2 * len(dataset))\n",
    "    test_size = len(dataset) - train_size - val_size\n",
    "    train_dataset, val_dataset, test_dataset = random_split(dataset, [train_size, val_size, test_size])\n",
    "    \n",
    "    # Create the data loaders\n",
    "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=32)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=32)\n",
    "    \n",
    "    # Define the model\n",
    "    num_classes = 10\n",
    "    model = TwoStreamCNN(num_classes)\n",
    "    \n",
    "    # Define the loss function and optimizer\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    \n",
    "    # Training loop\n",
    "    num_epochs = 10\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        \n",
    "        for batch in train_loader:\n",
    "            frames = batch\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(frames)\n",
    "            \n",
    "            # Compute the loss\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            # Backward pass and optimization\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item() * frames.size(0)\n",
    "        \n",
    "        # Compute the average training loss for the epoch\n",
    "        train_loss /= len(train_loader.dataset)\n",
    "        \n",
    "        # Validation loop\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_accuracy = 0.0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                frames = batch\n",
    "                \n",
    "                # Forward pass\n",
    "                outputs = model(frames)\n",
    "                \n",
    "                # Compute the loss\n",
    "                loss = criterion(outputs, labels)\n",
    "                \n",
    "                val_loss += loss.item() * frames.size(0)\n",
    "                val_accuracy += (outputs.argmax(dim=1) == labels).sum().item()\n",
    "        \n",
    "        # Compute the average validation loss and accuracy for the epoch\n",
    "        val_loss /= len(val_loader.dataset)\n",
    "        val_accuracy /= len(val_loader.dataset)\n",
    "        \n",
    "        # Print the training and validation metrics for the epoch\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}: Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}\")\n",
    "    \n",
    "    # Evaluation loop\n",
    "    model.eval()\n",
    "    test_loss = 0.0\n",
    "    test_accuracy = 0.0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            frames = batch\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(frames)\n",
    "            \n",
    "            # Compute the loss\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            test_loss += loss.item() * frames.size(0)\n",
    "            test_accuracy += (outputs.argmax(dim=1) == labels).sum().item()\n",
    "    \n",
    "    # Compute the average test loss and accuracy\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    test_accuracy /= len(test_loader.dataset)\n",
    "    \n",
    "    # Print the test metrics\n",
    "    print(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
